{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import pathlib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "from upsetplot import UpSet, from_memberships\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "except ImportError:\n",
    "    KaplanMeierFitter = CoxPHFitter = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.distance import Levenshtein\n",
    "except ImportError:\n",
    "    Levenshtein = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67e8ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = pathlib.Path(\".\").resolve().parent\n",
    "GRAPHS_DIR = ROOT / \"graphs\"\n",
    "PROC_DIR = ROOT / \"processed\"\n",
    "FIG_DIR = ROOT / \"figures\" / \"networks\"\n",
    "HTML_DIR = FIG_DIR / \"html\"\n",
    "\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "HTML_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLOTLY_TEMPL = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bfcd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(name: str) -> nx.Graph:\n",
    "    \"\"\"Load a GraphML file and return a NetworkX graph.\"\"\"\n",
    "    path = GRAPHS_DIR / f\"{name}.graphml\"\n",
    "    return nx.read_graphml(path)\n",
    "\n",
    "\n",
    "def add_cluster_attribute(G: nx.Graph, resolution: float = 1.0, attr: str = \"cluster\"):\n",
    "    \"\"\"Add Louvain community IDs as a node attribute.\"\"\"\n",
    "    if community_louvain is None:\n",
    "        raise ImportError(\"python-louvain is not installed in this environment.\")\n",
    "    partition = community_louvain.best_partition(nx.Graph(G), resolution=resolution)\n",
    "    nx.set_node_attributes(G, partition, attr)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13a98be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_flow_map(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"repost_flow.html\", max_nodes: int | None = None\n",
    "):\n",
    "    \"\"\"Interactive PyVis network for the repost-flow graph.\"\"\"\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    if max_nodes is not None and G.number_of_nodes() > max_nodes:\n",
    "        deg = dict(G.degree(weight=\"weight\"))\n",
    "        top_nodes = {\n",
    "            n\n",
    "            for n, _ in sorted(deg.items(), key=lambda kv: kv[1], reverse=True)[\n",
    "                :max_nodes\n",
    "            ]\n",
    "        }\n",
    "        G = G.subgraph(top_nodes).copy()\n",
    "\n",
    "    net = Network(height=\"750px\", width=\"100%\", directed=True, notebook=False)\n",
    "    net.from_nx(G)\n",
    "    net.show_buttons(filter_=[\"physics\"])\n",
    "    net.save_graph(str(html_out))\n",
    "    print(f\"✔ Interactive flow map → {html_out}\")\n",
    "\n",
    "\n",
    "def inout_scatter(plot_out: pathlib.Path = HTML_DIR / \"inout_scatter.html\"):\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"subreddit\": list(G.nodes()),\n",
    "            \"in_degree\": [G.in_degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"out_degree\": [G.out_degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"strength\": [G.degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"cluster\": [G.nodes[n][\"cluster\"] for n in G.nodes()],\n",
    "        }\n",
    "    )\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"out_degree\",\n",
    "        y=\"in_degree\",\n",
    "        color=\"cluster\",\n",
    "        size=\"strength\",\n",
    "        hover_name=\"subreddit\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"In‑ vs Out‑degree\",\n",
    "    )\n",
    "    fig.write_html(plot_out)\n",
    "    print(f\"✔ Scatter → {plot_out}\")\n",
    "\n",
    "\n",
    "def edge_weight_zipf(plot_out: pathlib.Path = FIG_DIR / \"edge_weight_zipf.png\"):\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    weights = sorted([d[\"weight\"] for _, _, d in G.edges(data=True)], reverse=True)\n",
    "    ranks = np.arange(1, len(weights) + 1)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.loglog(ranks, weights, marker=\".\")\n",
    "    plt.xlabel(\"Edge rank (log)\")\n",
    "    plt.ylabel(\"Edge weight (log)\")\n",
    "    plt.title(\"Zipf plot of edge weights\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Zipf plot → {plot_out}\")\n",
    "\n",
    "\n",
    "def sankey_top_images(\n",
    "    n_images: int = 10, html_out: pathlib.Path = HTML_DIR / \"sankey_images.html\"\n",
    "):\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    top_imgs = (\n",
    "        df.groupby(\"image_id\").size().sort_values(ascending=False).head(n_images).index\n",
    "    )\n",
    "    records = []\n",
    "    for img in top_imgs:\n",
    "        hops = df[df.image_id == img].sort_values(\"unixtime\")[\"subreddit\"].tolist()[:3]\n",
    "        if len(hops) == 3:\n",
    "            records.append(hops)\n",
    "    if not records:\n",
    "        print(\"⚠ No images with ≥3 hops – skip Sankey.\")\n",
    "        return\n",
    "    nodes = sorted({s for seq in records for s in seq})\n",
    "    nidx = {s: i for i, s in enumerate(nodes)}\n",
    "    src, dst, val = [], [], []\n",
    "    for seq in records:\n",
    "        for a, b in zip(seq, seq[1:]):\n",
    "            src.append(nidx[a])\n",
    "            dst.append(nidx[b])\n",
    "            val.append(1)\n",
    "    fig = go.Figure(\n",
    "        go.Sankey(\n",
    "            node=dict(label=nodes, pad=10), link=dict(source=src, target=dst, value=val)\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(title=\"Repost paths for top images\", template=PLOTLY_TEMPL)\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Sankey → {html_out}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2 · CO‑REPOST PROJECTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def chord_diagram(out=HTML_DIR / \"chord_corepost.html\", thr=50):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    E = [(u, v, d[\"weight\"]) for u, v, d in G.edges(data=True) if d[\"weight\"] >= thr]\n",
    "    if not E:\n",
    "        print(\"threshold too high – no edges\")\n",
    "        return\n",
    "    labs = sorted({u for u, _, _ in E} | {v for _, v, _ in E})\n",
    "    mat = np.zeros((n := len(labs), n))\n",
    "    id = {l: i for i, l in enumerate(labs)}\n",
    "    [\n",
    "        (lambda i, j, w: (mat.__setitem__((i, j), w), mat.__setitem__((j, i), w)))(\n",
    "            id[u], id[v], w\n",
    "        )\n",
    "        for u, v, w in E\n",
    "    ]\n",
    "    # try native Chord first (plotly ≥5.15)\n",
    "    if hasattr(go, \"Chord\"):\n",
    "        fig = go.Figure(go.Chord(labels=labs, matrix=mat.tolist()))\n",
    "    else:  # fallback thin‑arc sankey\n",
    "        src, dst, val = [], [], []\n",
    "        [src.append(id[u]) or dst.append(id[v]) or val.append(w) for u, v, w in E]\n",
    "        fig = go.Figure(\n",
    "            go.Sankey(\n",
    "                arrangement=\"fixed\",\n",
    "                node=dict(label=labs, pad=7, thickness=8),\n",
    "                link=dict(source=src, target=dst, value=val),\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=f\"Co‑repost diagram (≥{thr} shared images)\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "    )\n",
    "    fig.write_html(out)\n",
    "\n",
    "\n",
    "def upset_corepost(\n",
    "    plot_out: pathlib.Path = FIG_DIR / \"upset_corepost.png\", top_k: int = 12\n",
    "):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    strength = {n: G.degree(n, weight=\"weight\") for n in G.nodes()}\n",
    "    top_subs = [\n",
    "        n\n",
    "        for n, _ in sorted(strength.items(), key=lambda kv: kv[1], reverse=True)[:top_k]\n",
    "    ]\n",
    "    imgdf = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")[\n",
    "        [\"image_id\", \"subreddit\"]\n",
    "    ]\n",
    "    memberships = (\n",
    "        imgdf[imgdf.subreddit.isin(top_subs)]\n",
    "        .groupby(\"image_id\")[\"subreddit\"]\n",
    "        .apply(list)\n",
    "        .tolist()\n",
    "    )\n",
    "    upset = from_memberships(memberships)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    UpSet(upset, subset_size=\"count\", show_counts=True).plot()\n",
    "    plt.suptitle(\"UpSet – intersections across top subs\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ UpSet → {plot_out}\")\n",
    "\n",
    "\n",
    "def blockmodel_heatmap(plot_out: pathlib.Path = FIG_DIR / \"block_heatmap.png\"):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    part = add_cluster_attribute(G)\n",
    "    order = sorted(G.nodes(), key=lambda n: (part[n], G.degree(n, weight=\"weight\")))\n",
    "    idx = {n: i for i, n in enumerate(order)}\n",
    "    mat = np.zeros((len(order), len(order)))\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        i, j = idx[u], idx[v]\n",
    "        mat[i, j] = mat[j, i] = math.log1p(d[\"weight\"])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mat, cmap=\"mako_r\", xticklabels=False, yticklabels=False)\n",
    "    plt.title(\"Block‑model heat‑map (log1p weights)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Heat‑map → {plot_out}\")\n",
    "\n",
    "\n",
    "def sunburst_communities(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"sunburst_communities.html\",\n",
    "):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    partition = add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"subreddit\": list(partition.keys()),\n",
    "            \"cluster\": list(partition.values()),\n",
    "            \"strength\": [G.degree(n, weight=\"weight\") for n in partition],\n",
    "        }\n",
    "    )\n",
    "    fig = px.sunburst(\n",
    "        df,\n",
    "        path=[\"cluster\", \"subreddit\"],\n",
    "        values=\"strength\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        title=\"Community sunburst\",\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Sunburst → {html_out}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3 · REPOST‑AMPLIFICATION GRAPH\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def freq_vs_median_gain_scatter(out=HTML_DIR / \"freq_vs_gain.html\"):\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {**d, \"src\": u, \"dst\": v}\n",
    "            for u, v, d in load_graph(\"repost_amplification\").edges(data=True)\n",
    "        ]\n",
    "    )\n",
    "    size_col = df.mean_gain.abs().add(1)  # ensure >0\n",
    "    px.scatter(\n",
    "        df,\n",
    "        x=\"count\",\n",
    "        y=\"median_gain\",\n",
    "        size=size_col,\n",
    "        color=(df.median_gain > 0),\n",
    "        hover_data=[\"src\", \"dst\"],\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"Edge freq vs median karma gain (size = |mean_gain|+1)\",\n",
    "    ).write_html(out)\n",
    "\n",
    "\n",
    "def edge_rank_bump_chart(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"edge_bump.html\", top_k: int = 15\n",
    "):\n",
    "    # Approximate: rank subreddits by yearly median gain (source→any)\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    df = df.sort_values([\"image_id\", \"unixtime\"])\n",
    "    df[\"gain\"] = df.groupby(\"image_id\")[\"score\"].diff()\n",
    "    df = df.dropna(subset=[\"gain\"])\n",
    "    df[\"year\"] = pd.to_datetime(df.unixtime, unit=\"s\").dt.year\n",
    "    g = df.groupby([\"year\", \"subreddit\"])[\"gain\"].median().reset_index()\n",
    "    g[\"rank\"] = g.groupby(\"year\")[\"gain\"].rank(ascending=False, method=\"first\")\n",
    "    top = g[g[\"rank\"] <= top_k]\n",
    "    fig = px.line(\n",
    "        top,\n",
    "        x=\"year\",\n",
    "        y=\"rank\",\n",
    "        color=\"subreddit\",\n",
    "        hover_data=[\"gain\"],\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"Median‑gain rank evolution (top edges)\",\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Bump chart → {html_out}\")\n",
    "\n",
    "\n",
    "def exporter_bar_race(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"bar_race.html\", period: str = \"M\"\n",
    "):\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    df = df.sort_values([\"image_id\", \"unixtime\"])\n",
    "    df[\"gain\"] = df.groupby(\"image_id\")[\"score\"].diff()\n",
    "    df = df.dropna(subset=[\"gain\"])\n",
    "    df[\"period\"] = (\n",
    "        pd.to_datetime(df.unixtime, unit=\"s\").dt.to_period(period).dt.to_timestamp()\n",
    "    )\n",
    "    gains = df.groupby([\"period\", \"subreddit\"])[\"gain\"].mean().reset_index()\n",
    "    gains[\"cum_gain\"] = gains.groupby(\"subreddit\")[\"gain\"].cumsum()\n",
    "    fig = px.bar(\n",
    "        gains,\n",
    "        x=\"cum_gain\",\n",
    "        y=\"subreddit\",\n",
    "        animation_frame=\"period\",\n",
    "        orientation=\"h\",\n",
    "        range_x=[0, gains.cum_gain.max() * 1.05],\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"Cumulative avg karma exported per subreddit\",\n",
    "    )\n",
    "    fig.update_layout(yaxis={\"categoryorder\": \"total ascending\"})\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Bar‑race → {html_out}\")\n",
    "\n",
    "\n",
    "def resubmission_hist(out=FIG_DIR / \"resubmission_hist.png\"):\n",
    "    cnt = (\n",
    "        pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "        .groupby(\"image_id\")\n",
    "        .size()\n",
    "    )\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(cnt, bins=50, log_scale=(False, True))\n",
    "    plt.xlabel(\"# resubmissions per image\")\n",
    "    plt.ylabel(\"Images (log)\")\n",
    "    plt.title(\"Distribution of image resubmissions\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d3f50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FUNCS = [\n",
    "    interactive_flow_map,\n",
    "    inout_scatter,\n",
    "    edge_weight_zipf,\n",
    "    sankey_top_images,\n",
    "    chord_diagram,\n",
    "    upset_corepost,\n",
    "    blockmodel_heatmap,\n",
    "    sunburst_communities,\n",
    "    freq_vs_median_gain_scatter,\n",
    "    edge_rank_bump_chart,\n",
    "    exporter_bar_race,\n",
    "    resubmission_hist,\n",
    "]\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start = datetime.now()\n",
    "        for fn in ALL_FUNCS:\n",
    "            fname = fn.__name__\n",
    "            try:\n",
    "                print(f\"→ {fname}()\")\n",
    "                if fname == \"sankey_top_images\":\n",
    "                    fn(n_images=10)\n",
    "                else:\n",
    "                    fn()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ {fname} failed: {e}\")\n",
    "        print(\n",
    "            f\"Completed in {datetime.now() - start} – outputs in {FIG_DIR} & {HTML_DIR}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb053b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ interactive_flow_map()\n",
      "✔ Interactive flow map → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\repost_flow.html\n",
      "→ inout_scatter()\n",
      "✔ Scatter → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\inout_scatter.html\n",
      "→ edge_weight_zipf()\n",
      "✔ Zipf plot → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\edge_weight_zipf.png\n",
      "→ sankey_top_images()\n",
      "✔ Sankey → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\sankey_images.html\n",
      "→ chord_diagram()\n",
      "→ upset_corepost()\n",
      "✔ UpSet → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\upset_corepost.png\n",
      "→ blockmodel_heatmap()\n",
      "✔ Heat‑map → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\block_heatmap.png\n",
      "→ sunburst_communities()\n",
      "✔ Sunburst → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\sunburst_communities.html\n",
      "→ freq_vs_median_gain_scatter()\n",
      "→ edge_rank_bump_chart()\n",
      "✔ Bump chart → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\edge_bump.html\n",
      "→ exporter_bar_race()\n",
      "✔ Bar‑race → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\bar_race.html\n",
      "→ resubmission_hist()\n",
      "Completed in 0:00:09.674898 – outputs in C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks & C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
