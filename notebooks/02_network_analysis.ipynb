{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "7f36d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import pathlib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from upsetplot import UpSet, from_memberships\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "except ImportError:\n",
    "    KaplanMeierFitter = CoxPHFitter = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.distance import Levenshtein\n",
    "except ImportError:\n",
    "    Levenshtein = None\n",
    "\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "67e8ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = pathlib.Path(\".\").resolve().parent\n",
    "GRAPHS_DIR = ROOT / \"graphs\"\n",
    "PROC_DIR = ROOT / \"processed\"\n",
    "FIG_DIR = ROOT / \"figures\" / \"networks\"\n",
    "HTML_DIR = FIG_DIR / \"html\"\n",
    "\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "HTML_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLOTLY_TEMPL = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3bfcd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(name: str) -> nx.Graph:\n",
    "    \"\"\"Load a GraphML file and return a NetworkX graph.\"\"\"\n",
    "    path = GRAPHS_DIR / f\"{name}.graphml\"\n",
    "    return nx.read_graphml(path)\n",
    "\n",
    "\n",
    "def add_cluster_attribute(G: nx.Graph, resolution: float = 1.0, attr: str = \"cluster\"):\n",
    "    \"\"\"Add Louvain community IDs as a node attribute.\"\"\"\n",
    "    if community_louvain is None:\n",
    "        raise ImportError(\"python-louvain is not installed in this environment.\")\n",
    "    partition = community_louvain.best_partition(nx.Graph(G), resolution=resolution)\n",
    "    nx.set_node_attributes(G, partition, attr)\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "78190dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_flow_map(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"repost_flow.html\",\n",
    "    *,\n",
    "    layout_k=0.05,  # pack nodes a bit tighter\n",
    "    node_size_prop=5,  # shrink markers\n",
    "    edge_opacity=0.2,  # lighten but still visible\n",
    "    node_opacity=0.8,\n",
    "):  # make nodes stand out\n",
    "    # 2) Build edge traces\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    pos = nx.spring_layout(G, k=layout_k, seed=42)\n",
    "\n",
    "    # 2) Edge trace\n",
    "    edge_x, edge_y = [], []\n",
    "    for u, v, attrs in G.edges(data=True):\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x,\n",
    "        y=edge_y,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgba(100,100,100,{})\".format(edge_opacity), width=1),\n",
    "        hoverinfo=\"none\",\n",
    "    )\n",
    "\n",
    "    # 3) Node trace\n",
    "    node_x, node_y, node_text, node_deg = [], [], [], []\n",
    "    for node, deg in G.degree(weight=\"weight\"):\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        node_deg.append(deg)\n",
    "        node_text.append(f\"{node}<br>deg: {deg}\")\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x,\n",
    "        y=node_y,\n",
    "        mode=\"markers\",\n",
    "        hoverinfo=\"text\",\n",
    "        text=node_text,\n",
    "        marker=dict(\n",
    "            showscale=True,\n",
    "            colorscale=\"YlGnBu\",\n",
    "            color=node_deg,\n",
    "            size=[(d**0.5) * node_size_prop for d in node_deg],\n",
    "            opacity=node_opacity,\n",
    "            line=dict(width=0.5, color=\"black\"),\n",
    "            colorbar=dict(title=\"Weighted degree\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 4) Build & export\n",
    "    fig = go.Figure([edge_trace, node_trace])\n",
    "    fig.update_layout(\n",
    "        title=\"Repost-Flow (Plotly)\",\n",
    "        plot_bgcolor=\"white\",\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        margin=dict(l=20, r=20, t=40, b=20),\n",
    "    )\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(f\"✔ Wrote improved Plotly map → {html_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f7e2f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inout_scatter(plot_out: pathlib.Path = HTML_DIR / \"inout_scatter.html\"):\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"subreddit\": list(G.nodes()),\n",
    "            \"in_degree\": [G.in_degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"out_degree\": [G.out_degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"strength\": [G.degree(n, weight=\"weight\") for n in G.nodes()],\n",
    "            \"cluster\": [G.nodes[n][\"cluster\"] for n in G.nodes()],\n",
    "        }\n",
    "    )\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"out_degree\",\n",
    "        y=\"in_degree\",\n",
    "        color=\"cluster\",\n",
    "        size=\"strength\",\n",
    "        hover_name=\"subreddit\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"In‑ vs Out‑degree (log–log)\",\n",
    "    )\n",
    "    fig.update_xaxes(type=\"log\")\n",
    "    fig.update_yaxes(type=\"log\")\n",
    "    fig.update_layout(legend_title_text=\"Community cluster\")\n",
    "    fig.write_html(plot_out)\n",
    "    print(f\"✔ Scatter → {plot_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739aa28",
   "metadata": {},
   "source": [
    "## Zipf plot of edge weights\n",
    "\n",
    "### 1) What the plot does\n",
    "\n",
    "Ranks every subreddit-to-subreddit edge by the number of images that traversed it and plots rank versus weight on doubly-log scales.  \n",
    "_Why we usually make this plot_ – Zipf curves reveal whether a few diffusion channels dominate and how sharply traffic drops off.\n",
    "\n",
    "### 2) What we observe in it\n",
    "\n",
    "• Near-linear negative slope → classic heavy-tailed behaviour.  \n",
    "• Top five edges transmit orders of magnitude more images than the median edge.  \n",
    "• A shoulder around ranks 5–15 hints at a “core highway” of medium-volume routes.  \n",
    "• Tail flattens gently; no abrupt elbow or regime change.\n",
    "\n",
    "### 3) Insights relative to the pitch\n",
    "\n",
    "• Validates focusing on high-capacity “express lanes” when hunting revival hubs.  \n",
    "• Long tail of low-traffic edges suggests niche subs occasionally give images a second life – good material for case studies.  \n",
    "• Confirms we can prune the network for visual clarity without losing explanatory power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c955c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_weight_zipf(plot_out: pathlib.Path = FIG_DIR / \"edge_weight_zipf.png\"):\n",
    "    G = load_graph(\"repost_flow\")\n",
    "    weights = sorted([d[\"weight\"] for _, _, d in G.edges(data=True)], reverse=True)\n",
    "    ranks = np.arange(1, len(weights) + 1)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.loglog(ranks, weights, marker=\".\")\n",
    "    plt.xlabel(\"Edge rank (log)\")\n",
    "    plt.ylabel(\"Edge weight (log)\")\n",
    "    plt.title(\"Zipf plot of edge weights\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Zipf plot → {plot_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "9d22e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sankey_top_images(\n",
    "    *, n_images: int = 10, html_out: pathlib.Path = HTML_DIR / \"sankey_images.html\"\n",
    "):\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    top_imgs = (\n",
    "        df.groupby(\"image_id\").size().sort_values(ascending=False).head(n_images).index\n",
    "    )\n",
    "    paths: list[list[str]] = []\n",
    "    for img in top_imgs:\n",
    "        hops = df[df.image_id == img].sort_values(\"unixtime\")[\"subreddit\"].tolist()[:3]\n",
    "        if len(hops) == 3:\n",
    "            paths.append(hops)\n",
    "    if not paths:\n",
    "        print(\"⚠ No images with ≥3 hops – skip Sankey.\")\n",
    "        return\n",
    "    nodes = sorted({s for seq in paths for s in seq})\n",
    "    nidx = {s: i for i, s in enumerate(nodes)}\n",
    "    link_df = (\n",
    "        pd.DataFrame(\n",
    "            [(nidx[a], nidx[b]) for seq in paths for a, b in zip(seq, seq[1:])],\n",
    "            columns=[\"src\", \"dst\"],\n",
    "        )\n",
    "        .assign(val=1)\n",
    "        .groupby([\"src\", \"dst\"], as_index=False)\n",
    "        .val.sum()\n",
    "    )\n",
    "    # Consistent link colour based on *destination* community bucket\n",
    "    palette = px.colors.qualitative.Plotly\n",
    "    link_df[\"color\"] = link_df.dst.apply(lambda d: palette[d % len(palette)])\n",
    "    node_colors = [palette[i % len(palette)] for i in range(len(nodes))]\n",
    "    fig = go.Figure(\n",
    "        go.Sankey(\n",
    "            node=dict(label=nodes, pad=12, thickness=12, color=node_colors),\n",
    "            link=dict(\n",
    "                source=link_df.src,\n",
    "                target=link_df.dst,\n",
    "                value=link_df.val,\n",
    "                color=link_df.color,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=\"Repost paths for top‑circulated images\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Sankey → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5d9d82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chord_diagram(\n",
    "    *, out: pathlib.Path = HTML_DIR / \"chord_corepost.html\", thr: int = 50\n",
    "):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    edges = [\n",
    "        (u, v, d[\"weight\"]) for u, v, d in G.edges(data=True) if d[\"weight\"] >= thr\n",
    "    ]\n",
    "    if not edges:\n",
    "        print(\"threshold too high – no edges\")\n",
    "        return\n",
    "\n",
    "    labels = sorted({u for u, _, _ in edges} | {v for _, v, _ in edges})\n",
    "    n = len(labels)\n",
    "    idx = {l: i for i, l in enumerate(labels)}\n",
    "    matrix = np.zeros((n, n))\n",
    "    for u, v, w in edges:\n",
    "        i, j = idx[u], idx[v]\n",
    "        matrix[i, j] = matrix[j, i] = w  # ensure symmetry\n",
    "\n",
    "    if hasattr(go, \"Chord\"):\n",
    "        fig = go.Figure(go.Chord(labels=labels, matrix=matrix.tolist()))\n",
    "    else:\n",
    "        src = [idx[u] for u, v, _ in edges]\n",
    "        dst = [idx[v] for u, v, _ in edges]\n",
    "        val = [w for _, _, w in edges]\n",
    "        fig = go.Figure(\n",
    "            go.Sankey(\n",
    "                arrangement=\"fixed\",\n",
    "                node=dict(label=labels, pad=15, thickness=10),  # pad enlarged\n",
    "                link=dict(source=src, target=dst, value=val),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Co‑repost diagram (≥{thr} shared images)\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "    )\n",
    "    fig.write_html(out)\n",
    "    print(f\"✔ Chord → {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d24c0c",
   "metadata": {},
   "source": [
    "## UpSet plot – multi-subreddit image paths\n",
    "\n",
    "### 1) What the plot does\n",
    "\n",
    "UpSet visualisation of the most frequent repost paths: top bar = number of images following a specific path; dot-matrix = which subreddits form that path.  \n",
    "_Why we usually make this plot_ – UpSet handles many-set intersections cleanly, exposing popular multi-hop trajectories without overplotting.\n",
    "\n",
    "### 2) What we observe in it\n",
    "\n",
    "• r/funny, r/pics and r/aww dominate the tallest bars – they are the final destination for most recycled images.  \n",
    "• Most paths are two hops; longer chains drop sharply in frequency.  \n",
    "• Common pattern: niche or topic sub → general-interest sub → mass-appeal hub.  \n",
    "• Directionality is clear – paths rarely flow back to the niche origin.  \n",
    "• Even rare, tail-end paths converge on the same revival hubs, underscoring their gravitational pull.\n",
    "\n",
    "### 3) Insights relative to the pitch\n",
    "\n",
    "• Empirically demonstrates the “promotion ladder” from niche to mainstream communities.  \n",
    "• Reinforces that changing _where_ an image is posted can be as powerful as renaming it.  \n",
    "• Provides concrete trajectories we can animate or narrate in the final story, linking the static and network chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b9c2e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upset_corepost(\n",
    "    plot_out: pathlib.Path = FIG_DIR / \"upset_corepost.png\", top_k: int = 12\n",
    "):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    strength = {n: G.degree(n, weight=\"weight\") for n in G.nodes()}\n",
    "    top_subs = [\n",
    "        n\n",
    "        for n, _ in sorted(strength.items(), key=lambda kv: kv[1], reverse=True)[:top_k]\n",
    "    ]\n",
    "    imgdf = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")[\n",
    "        [\"image_id\", \"subreddit\"]\n",
    "    ]\n",
    "    memberships = (\n",
    "        imgdf[imgdf.subreddit.isin(top_subs)]\n",
    "        .groupby(\"image_id\")[\"subreddit\"]\n",
    "        .apply(list)\n",
    "        .tolist()\n",
    "    )\n",
    "    upset = from_memberships(memberships)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    UpSet(upset, subset_size=\"count\", show_counts=True).plot()\n",
    "    plt.suptitle(\"UpSet – intersections across top subs\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ UpSet → {plot_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e768a2",
   "metadata": {},
   "source": [
    "## Block-model adjacency heat-maps (log1p weights)\n",
    "\n",
    "### 1) What the plot does\n",
    "\n",
    "Four zoomed adjacency matrices of the repost-flow graph, reordered by community detection (Louvain / SBM) and coloured by the log of edge weight.  \n",
    "_Why we usually make this plot_ – to visually confirm modular structure and spot dense intra-cluster vs sparse inter-cluster connections.\n",
    "\n",
    "### 2) What we observe in them\n",
    "\n",
    "• Dark squares along the diagonal show strong reposting within thematic clusters.  \n",
    "• Thin bright stripes bridging blocks highlight high-traffic cross-community links.  \n",
    "• Blocks vary in density and size – some subs act as large generalist hubs, others as tight niche clusters.  \n",
    "• Occasional bright pixels outside any block reveal individual “wormholes” (e.g., r/funny → a niche sub) that shortcut the hierarchy.\n",
    "\n",
    "### 3) Insights relative to the pitch\n",
    "\n",
    "• Supports the idea that revival mostly happens _inside_ communities before an image jumps outward.  \n",
    "• Cross-block stripes identify prime export hubs for the amplification chapter.  \n",
    "• No obvious artefacts – block structure appears data-driven, so later path analyses rest on solid ground.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d8675312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockmodel_heatmap(plot_out: pathlib.Path = FIG_DIR / \"block_heatmap.png\"):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    part = add_cluster_attribute(G)\n",
    "    order = sorted(G.nodes(), key=lambda n: (part[n], G.degree(n, weight=\"weight\")))\n",
    "    idx = {n: i for i, n in enumerate(order)}\n",
    "    mat = np.zeros((len(order), len(order)))\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        i, j = idx[u], idx[v]\n",
    "        mat[i, j] = mat[j, i] = math.log1p(d[\"weight\"])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mat, cmap=\"mako_r\", xticklabels=False, yticklabels=False)\n",
    "    plt.title(\"Block‑model heat‑map (log1p weights)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Heat‑map → {plot_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "73063682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sunburst_communities(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"sunburst_communities.html\",\n",
    "):\n",
    "    G = load_graph(\"corepost_projection\")\n",
    "    partition = add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"subreddit\": list(partition.keys()),\n",
    "            \"cluster\": list(partition.values()),\n",
    "            \"strength\": [G.degree(n, weight=\"weight\") for n in partition],\n",
    "        }\n",
    "    )\n",
    "    df[\"all\"] = \"All\"\n",
    "    fig = px.sunburst(\n",
    "        df,\n",
    "        path=[\"all\", \"cluster\", \"subreddit\"],\n",
    "        values=\"strength\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        title=\"Community sunburst\",\n",
    "        height=700,\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Sunburst → {html_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "fe6211bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_vs_median_gain_scatter(out: pathlib.Path = HTML_DIR / \"freq_vs_gain.html\"):\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {**d, \"src\": u, \"dst\": v}\n",
    "            for u, v, d in load_graph(\"repost_amplification\").edges(data=True)\n",
    "        ]\n",
    "    )\n",
    "    df[\"count\"] = pd.to_numeric(df[\"count\"], errors=\"coerce\")\n",
    "    df[\"median_gain\"] = pd.to_numeric(df[\"median_gain\"], errors=\"coerce\")\n",
    "    df[\"mean_gain\"] = pd.to_numeric(df[\"mean_gain\"], errors=\"coerce\")\n",
    "    size_col = df.mean_gain.abs() + 1\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"count\",\n",
    "        y=\"median_gain\",\n",
    "        size=size_col,\n",
    "        color=(df.median_gain > 0),\n",
    "        hover_data=[\"src\", \"dst\", \"mean_gain\"],\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"Edge frequency vs. median karma gain (marker size ∝ |mean_gain|)\",\n",
    "    )\n",
    "    fig.update_xaxes(type=\"log\")\n",
    "    fig.write_html(out)\n",
    "    print(f\"✔ Freq‑vs‑gain scatter → {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6c6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1243f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc9f83a",
   "metadata": {},
   "source": [
    "## Distribution of image resubmissions\n",
    "\n",
    "### 1) What the plot does\n",
    "\n",
    "A histogram of how often each image re-appears in the dataset, with counts shown on a logarithmic scale.  \n",
    "_Why we usually make this plot_ – to check whether content popularity is narrowly concentrated or has a long-tailed “viral” spread.\n",
    "\n",
    "### 2) What we observe in it\n",
    "\n",
    "• A dominant spike at “single-use” images – most pictures are never reposted.  \n",
    "• A smooth, heavy-tailed decline stretching past 150 resubmissions.  \n",
    "• No clear secondary hump, indicating reposting frequency is continuous rather than bimodal.  \n",
    "• Only a handful of extreme outliers (“evergreens”) account for the far right tail.\n",
    "\n",
    "### 3) Insights relative to the pitch\n",
    "\n",
    "• Confirms the _“one image, many faces”_ framing – a tiny minority fuels the recycling story.  \n",
    "• Suggests revival hubs are choosy: they amplify just a sliver of all content.  \n",
    "• Provides a baseline for later survival-curve work (temporal chapter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "13a98be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resubmission_hist(out=FIG_DIR / \"resubmission_hist.png\"):\n",
    "    cnt = (\n",
    "        pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "        .groupby(\"image_id\")\n",
    "        .size()\n",
    "    )\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(cnt, bins=50, log_scale=(False, True))\n",
    "    plt.xlabel(\"# resubmissions per image\")\n",
    "    plt.ylabel(\"Images (log)\")\n",
    "    plt.title(\"Distribution of image resubmissions\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FUNCS = [\n",
    "    plotly_flow_map,\n",
    "    inout_scatter,\n",
    "    edge_weight_zipf,\n",
    "    sankey_top_images,\n",
    "    chord_diagram,\n",
    "    upset_corepost,\n",
    "    blockmodel_heatmap,\n",
    "    sunburst_communities,\n",
    "    freq_vs_median_gain_scatter,\n",
    "    resubmission_hist,\n",
    "]\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start = datetime.now()\n",
    "        for fn in ALL_FUNCS:\n",
    "            fname = fn.__name__\n",
    "            try:\n",
    "                print(f\"→ {fname}()\")\n",
    "                if fname == \"sankey_top_images\":\n",
    "                    fn(n_images=10)\n",
    "                else:\n",
    "                    fn()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ {fname} failed: {e}\")\n",
    "        print(\n",
    "            f\"Completed in {datetime.now() - start} – outputs in {FIG_DIR} & {HTML_DIR}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "eb053b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ plotly_flow_map()\n",
      "✔ Wrote improved Plotly map → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\repost_flow.html\n",
      "→ inout_scatter()\n",
      "✔ Scatter → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\inout_scatter.html\n",
      "→ edge_weight_zipf()\n",
      "✔ Zipf plot → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\edge_weight_zipf.png\n",
      "→ sankey_top_images()\n",
      "✔ Sankey → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\sankey_images.html\n",
      "→ chord_diagram()\n",
      "✔ Chord → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\chord_corepost.html\n",
      "→ upset_corepost()\n",
      "✔ UpSet → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\upset_corepost.png\n",
      "→ blockmodel_heatmap()\n",
      "✔ Heat‑map → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\block_heatmap.png\n",
      "→ sunburst_communities()\n",
      "✔ Sunburst → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\sunburst_communities.html\n",
      "→ freq_vs_median_gain_scatter()\n",
      "✔ Freq‑vs‑gain scatter → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\freq_vs_gain.html\n",
      "→ resubmission_hist()\n",
      "Completed in 0:00:13.449315 – outputs in C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks & C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
