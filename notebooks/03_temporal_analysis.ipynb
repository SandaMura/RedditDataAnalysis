{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf2b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a190b128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pathlib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "except ImportError:\n",
    "    KaplanMeierFitter = CoxPHFitter = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.distance import Levenshtein\n",
    "except ImportError:\n",
    "    Levenshtein = None\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "except ImportError:\n",
    "    KaplanMeierFitter = CoxPHFitter = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.distance import Levenshtein\n",
    "except ImportError:\n",
    "    Levenshtein = None\n",
    "\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a889abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = pathlib.Path(\".\").resolve().parent\n",
    "GRAPHS_DIR = ROOT / \"graphs\"\n",
    "PROC_DIR = ROOT / \"processed\"\n",
    "FIG_DIR = ROOT / \"figures\" / \"temporal\"\n",
    "HTML_DIR = FIG_DIR / \"html\"\n",
    "\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "HTML_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLOTLY_TEMPL = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0139e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(name: str) -> nx.Graph:\n",
    "    path = GRAPHS_DIR / f\"{name}.graphml\"\n",
    "    return nx.read_graphml(path)\n",
    "\n",
    "\n",
    "def add_cluster_attribute(G: nx.Graph, resolution: float = 1.0, attr: str = \"cluster\"):\n",
    "    if community_louvain is None:\n",
    "        raise ImportError(\"python-louvain is not installed in this environment.\")\n",
    "    partition = community_louvain.best_partition(nx.Graph(G), resolution=resolution)\n",
    "    nx.set_node_attributes(G, partition, attr)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def load_hops() -> pd.DataFrame:\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\").sort_values(\n",
    "        [\"image_id\", \"unixtime\"]\n",
    "    )\n",
    "    df[\"next_time\"] = df.groupby(\"image_id\").unixtime.shift(-1)\n",
    "    df[\"next_sub\"] = df.groupby(\"image_id\").subreddit.shift(-1)\n",
    "    hops = df.dropna(subset=[\"next_time\"]).loc[df.subreddit != df.next_sub].copy()\n",
    "    hops[\"gap_h\"] = (hops.next_time - hops.unixtime) / 3600.0\n",
    "    first_year = pd.to_datetime(\n",
    "        df.groupby(\"image_id\").unixtime.transform(\"first\"), unit=\"s\"\n",
    "    ).dt.year\n",
    "    hops[\"first_year\"] = first_year\n",
    "    hops[\"gap_idx\"] = hops.groupby(\"image_id\").cumcount() + 1\n",
    "    hops[\"timestamp\"] = pd.to_datetime(hops.next_time, unit=\"s\")\n",
    "    return hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9312abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_carpet(html_out: pathlib.Path = HTML_DIR / \"speed_carpet.html\"):\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"src\": u,\n",
    "                \"dst\": v,\n",
    "                \"speed\": d[\"speed\"],\n",
    "                \"src_cl\": G.nodes[u][\"cluster\"],\n",
    "                \"dst_cl\": G.nodes[v][\"cluster\"],\n",
    "            }\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "    )\n",
    "    df_piv = df.pivot(index=\"src\", columns=\"dst\", values=\"speed\")\n",
    "    fig = px.imshow(\n",
    "        np.log10(df_piv.fillna(1e-6)),\n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale=\"Turbo\",\n",
    "        labels=dict(color=\"log10(speed)\"),\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=900,\n",
    "        title=\"Edge speed carpet (log10)\",\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Speed carpet → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac805d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_median_gap_series(out=HTML_DIR / \"rolling_gap.html\", window=\"30D\"):\n",
    "    hops = load_hops().set_index(\"timestamp\").sort_index()\n",
    "    med = hops.gap_h.rolling(window).median().dropna()\n",
    "    px.line(\n",
    "        med,\n",
    "        title=f\"Rolling {window} median repost gap\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=600,\n",
    "    ).update_yaxes(type=\"log\").write_html(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94a5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_resubmission_volume(out=HTML_DIR / \"monthly_resubs.html\"):\n",
    "    hops = load_hops()\n",
    "    hops[\"month\"] = hops.timestamp.dt.to_period(\"M\").dt.to_timestamp()\n",
    "    vol = hops.groupby(\"month\").size().reset_index(name=\"resubs\")\n",
    "    px.bar(\n",
    "        vol,\n",
    "        x=\"month\",\n",
    "        y=\"resubs\",\n",
    "        title=\"Monthly resubmission volume\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=600,\n",
    "    ).write_html(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81eb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exporter_bar_race(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"bar_race.html\", *, period: str = \"M\"\n",
    "):\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    df = df.sort_values([\"image_id\", \"unixtime\"])\n",
    "    df[\"gain\"] = df.groupby(\"image_id\")[\"score\"].diff()\n",
    "    df = df.dropna(subset=[\"gain\"])\n",
    "    df[\"period\"] = (\n",
    "        pd.to_datetime(df.unixtime, unit=\"s\").dt.to_period(period).dt.to_timestamp()\n",
    "    )\n",
    "\n",
    "    gains = df.groupby([\"period\", \"subreddit\"])[\"gain\"].mean().reset_index()\n",
    "    gains[\"cum_gain\"] = gains.groupby(\"subreddit\")[\"gain\"].cumsum()\n",
    "\n",
    "    fig = px.bar(\n",
    "        gains,\n",
    "        x=\"cum_gain\",\n",
    "        y=\"subreddit\",\n",
    "        animation_frame=\"period\",\n",
    "        orientation=\"h\",\n",
    "        range_x=[0, gains.cum_gain.max() * 1.05],\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=700,\n",
    "        title=\"Cumulative avg karma exported per subreddit\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis={\"categoryorder\": \"total ascending\"},\n",
    "        transition={\"duration\": 500},\n",
    "        # frame={\"duration\": 500},\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Bar‑race → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c531a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_rank_bump_chart(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"edge_bump.html\",\n",
    "    *,\n",
    "    top_k: int = 50,\n",
    "    min_years: int = 2,\n",
    "    palette: list[str] | None = None,\n",
    "):\n",
    "    # Load & prepare data\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\").sort_values(\n",
    "        [\"image_id\", \"unixtime\"]\n",
    "    )\n",
    "    df[\"gain\"] = df.groupby(\"image_id\")[\"score\"].diff()\n",
    "    df = df.dropna(subset=[\"gain\"])\n",
    "    df[\"year\"] = pd.to_datetime(df.unixtime, unit=\"s\").dt.year\n",
    "\n",
    "    # Median gain by (year, subreddit)\n",
    "    g = df.groupby([\"year\", \"subreddit\"])[\"gain\"].median().reset_index()\n",
    "    g[\"rank\"] = g.groupby(\"year\")[\"gain\"].rank(method=\"min\", ascending=False)\n",
    "\n",
    "    # Filter by top_k and frequency\n",
    "    eligible = g.loc[g[\"rank\"] <= top_k, \"subreddit\"].value_counts()\n",
    "    keep_subs = eligible[eligible >= min_years].index\n",
    "    g = g[g[\"subreddit\"].isin(keep_subs) & (g[\"rank\"] <= top_k)]\n",
    "\n",
    "    # Colour mapping\n",
    "    if palette is None:\n",
    "        palette = px.colors.qualitative.Plotly\n",
    "    colour_map = {\n",
    "        sub: palette[i % len(palette)] for i, sub in enumerate(sorted(keep_subs))\n",
    "    }\n",
    "\n",
    "    # Build figure\n",
    "    fig = go.Figure()\n",
    "    for sub in sorted(keep_subs):\n",
    "        d = g[g[\"subreddit\"] == sub].sort_values(\"year\")\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=d[\"year\"],\n",
    "                y=d[\"rank\"],\n",
    "                mode=\"lines+markers\",\n",
    "                line=dict(width=2, color=colour_map[sub]),\n",
    "                marker=dict(size=6),\n",
    "                name=sub,\n",
    "                hovertemplate=(\n",
    "                    f\"<b>{sub}</b>\"  # the bit you want to keep\n",
    "                    \"<br>year %{x}: rank %{y}\"\n",
    "                    \"<br>median gain %{customdata:+,.0f}\"\n",
    "                    \"<extra></extra>\"  # <-- suppress the duplicate on the right\n",
    "                ),\n",
    "                customdata=d[\"gain\"].values,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(autorange=\"reversed\", title=\"Rank (1 = highest median gain)\")\n",
    "    fig.update_xaxes(title=\"Year\", dtick=1)\n",
    "    fig.update_layout(\n",
    "        title=\"Median‑gain rank evolution of leading subreddits\",\n",
    "        height=600,\n",
    "        template=\"plotly_white\",\n",
    "        legend_title=\"Subreddit\",\n",
    "        margin=dict(l=60, r=20, t=60, b=40),\n",
    "    )\n",
    "\n",
    "    # Export\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(f\"✔ Bump chart → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e5ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_span_trend(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"attention_span.html\",\n",
    "):\n",
    "    hops = load_hops()\n",
    "    df = hops[[\"gap_h\", \"first_year\"]].copy()\n",
    "    df[\"log_gap\"] = np.log10(df.gap_h)\n",
    "\n",
    "    med = df.groupby(\"first_year\")[\"log_gap\"].median().reset_index(name=\"median_lg\")\n",
    "\n",
    "    trend = (\n",
    "        alt.Chart(med)\n",
    "        .mark_line(point=True, strokeWidth=3)\n",
    "        .encode(\n",
    "            x=\"first_year:O\",\n",
    "            y=alt.Y(\n",
    "                \"median_lg:Q\",\n",
    "                scale=alt.Scale(type=\"linear\"),\n",
    "                axis=alt.Axis(format=\"~s\"),\n",
    "            ),\n",
    "            tooltip=[\"first_year\", alt.Tooltip(\"median_lg\", title=\"Median (log10 h)\")],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chart = trend.properties(\n",
    "        width=550, height=400, title=\"Attention span shrinks 2008 → 2013\"\n",
    "    )\n",
    "    chart.save(str(html_out))\n",
    "    print(f\"✔ Attention-span chart → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8583c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_ccdf(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"gap_ccdf.html\",\n",
    "    cutoff_h: float | None = None,\n",
    "):\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    gaps = np.array([d[\"median_gap_h\"] for _, _, d in G.edges(data=True)])\n",
    "    x = np.sort(gaps)\n",
    "    y = 1.0 - np.arange(1, len(x) + 1) / len(x)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            mode=\"lines+markers\",\n",
    "            marker=dict(size=3),\n",
    "            name=\"CCDF\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=PLOTLY_TEMPL,\n",
    "        xaxis_title=\"Median repost gap (h, log)\",\n",
    "        yaxis_title=\"P(X ≥ x)\",\n",
    "        xaxis_type=\"log\",\n",
    "        yaxis_type=\"log\",\n",
    "        height=500,\n",
    "        title=\"Heavy-tail of repost latencies\",\n",
    "    )\n",
    "\n",
    "    if cutoff_h is None:\n",
    "        cutoff_h = np.quantile(gaps, 0.9)\n",
    "    fig.add_vline(\n",
    "        x=cutoff_h,\n",
    "        line=dict(dash=\"dash\"),\n",
    "        annotation_text=f\"90 % cut ≈ {cutoff_h:.0f} h\",\n",
    "    )\n",
    "\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ CCDF → {html_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69fcd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_heatmap_interactive(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"hour_heatmap.html\",\n",
    "):\n",
    "    hops = load_hops()\n",
    "    hops[\"src_hr\"] = pd.to_datetime(hops.unixtime, unit=\"s\").dt.hour\n",
    "    hops[\"dst_hr\"] = hops.timestamp.dt.hour\n",
    "\n",
    "    med = hops.pivot_table(\n",
    "        index=\"src_hr\", columns=\"dst_hr\", values=\"gap_h\", aggfunc=\"median\"\n",
    "    )\n",
    "    cnt = (\n",
    "        hops.pivot_table(\n",
    "            index=\"src_hr\", columns=\"dst_hr\", values=\"gap_h\", aggfunc=\"size\"\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    z = np.log10(med + 1e-3)\n",
    "    fig = px.imshow(\n",
    "        z,\n",
    "        aspect=\"auto\",\n",
    "        labels=dict(color=\"log10(median h)\"),\n",
    "        x=list(med.columns),\n",
    "        y=list(med.index),\n",
    "        template=PLOTLY_TEMPL,\n",
    "        title=\"When to repost: source-hour × destination-hour\",\n",
    "        height=700,\n",
    "        color_continuous_scale=\"Viridis\",\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        customdata=cnt.values,\n",
    "        hovertemplate=\"src %{y} h → dst %{x} h<br>median=%{z:.2f} log10 h<br>N=%{customdata} hops<extra></extra>\",\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Hour heat-map → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75a4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_interactive(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"survival_interactive.html\",\n",
    "):\n",
    "    if KaplanMeierFitter is None:\n",
    "        print(\"ℹ️ lifelines not installed – interactive survival skipped\")\n",
    "        return\n",
    "\n",
    "    hops = load_hops()\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    traces, buttons = [], []\n",
    "    visible_mask = []\n",
    "\n",
    "    def add_trace(label, x, y, group_type):\n",
    "        idx = len(visible_mask)\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                mode=\"lines\",\n",
    "                name=label,\n",
    "                visible=False,\n",
    "                hovertemplate=f\"{label}<br>t=%{{x:.1f}} h<br>survival=%{{y:.2f}}\",\n",
    "            )\n",
    "        )\n",
    "        visible_mask.append((group_type, idx))\n",
    "\n",
    "    for yr, grp in hops.groupby(\"first_year\"):\n",
    "        kmf.fit(grp.gap_h, event_observed=np.ones(len(grp)))\n",
    "        add_trace(\n",
    "            str(int(yr)),\n",
    "            kmf.survival_function_.index,\n",
    "            kmf.survival_function_[\"KM_estimate\"],\n",
    "            \"cohort\",\n",
    "        )\n",
    "\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    q1, q2 = np.quantile([d[\"speed\"] for _, _, d in G.edges(data=True)], [1 / 3, 2 / 3])\n",
    "    cls_lookup = {\n",
    "        (u, v): (\"slow\" if s <= q1 else \"fast\" if s >= q2 else \"mid\")\n",
    "        for u, v, d in G.edges(data=True)\n",
    "        for s in [d[\"speed\"]]\n",
    "    }\n",
    "    hops[\"edge\"] = list(zip(hops.subreddit, hops.next_sub))\n",
    "    hops[\"speed_class\"] = hops.edge.map(cls_lookup)\n",
    "\n",
    "    for spd, grp in hops.dropna(subset=[\"speed_class\"]).groupby(\"speed_class\"):\n",
    "        kmf.fit(grp.gap_h, event_observed=np.ones(len(grp)))\n",
    "        add_trace(\n",
    "            spd,\n",
    "            kmf.survival_function_.index,\n",
    "            kmf.survival_function_[\"KM_estimate\"],\n",
    "            \"speed\",\n",
    "        )\n",
    "\n",
    "    init_vis = [\"cohort\", \"speed\"]\n",
    "    menus = []\n",
    "    for gtype in init_vis:\n",
    "        mask = [vis[0] == gtype for vis in visible_mask]\n",
    "        menus.append(\n",
    "            dict(\n",
    "                label=f\"{gtype.capitalize()} groups\",\n",
    "                method=\"update\",\n",
    "                args=[{\"visible\": mask}, {\"title\": f\"Kaplan–Meier — {gtype}\"}],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig = go.Figure(traces)\n",
    "    fig.update_layout(\n",
    "        template=PLOTLY_TEMPL,\n",
    "        yaxis_title=\"Survival probability\",\n",
    "        xaxis_title=\"Hours (log)\",\n",
    "        xaxis_type=\"log\",\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type=\"dropdown\",\n",
    "                x=1.02,\n",
    "                y=0.9,\n",
    "                buttons=menus,\n",
    "            )\n",
    "        ],\n",
    "        title=\"Kaplan–Meier — cohort\",\n",
    "        height=500,\n",
    "    )\n",
    "    for tr, vis in zip(fig.data, visible_mask):\n",
    "        tr.visible = vis[0] == \"cohort\"\n",
    "\n",
    "    fig.write_html(html_out, include_plotlyjs=\"cdn\")\n",
    "    print(f\"✔ Interactive KM → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77f402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cox_coeff_bar_ci(\n",
    "    html_out: pathlib.Path = HTML_DIR / \"cox_coeffs_ci.html\",\n",
    "):\n",
    "    if CoxPHFitter is None or Levenshtein is None:\n",
    "        print(\"ℹ️ lifelines/rapidfuzz missing – Cox bar skipped\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\").sort_values(\n",
    "        [\"image_id\", \"unixtime\"]\n",
    "    )\n",
    "    df[\"next_time\"] = df.groupby(\"image_id\").unixtime.shift(-1)\n",
    "    df[\"next_title\"] = df.groupby(\"image_id\").title.shift(-1)\n",
    "    df[\"next_sub\"] = df.groupby(\"image_id\").subreddit.shift(-1)\n",
    "\n",
    "    hops = df.dropna(subset=[\"next_time\"]).loc[df.subreddit != df.next_sub]\n",
    "    hops[\"duration\"] = (hops.next_time - hops.unixtime) / 3600.0\n",
    "    hops[\"event\"] = 1\n",
    "\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    speed_lu = {(u, v): d[\"speed\"] for u, v, d in G.edges(data=True)}\n",
    "    hops[\"speed\"] = list(\n",
    "        map(lambda p: speed_lu.get(p, np.nan), zip(hops.subreddit, hops.next_sub))\n",
    "    )\n",
    "    hops = hops.dropna(subset=[\"speed\"])\n",
    "\n",
    "    hops[\"title_dist\"] = hops.apply(\n",
    "        lambda r: Levenshtein.distance(r.title, r.next_title), axis=1\n",
    "    )\n",
    "    hops[\"init_score\"] = hops.score\n",
    "\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(\n",
    "        hops[[\"duration\", \"event\", \"speed\", \"title_dist\", \"init_score\"]],\n",
    "        duration_col=\"duration\",\n",
    "        event_col=\"event\",\n",
    "    )\n",
    "\n",
    "    coef = cph.params_.rename(\"coef\").to_frame()\n",
    "    ci = cph.confidence_intervals_\n",
    "    coef[\"low\"] = ci.iloc[:, 0]\n",
    "    coef[\"high\"] = ci.iloc[:, 1]\n",
    "    coef = coef.sort_values(\"coef\")\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            x=coef[\"coef\"],\n",
    "            y=coef.index,\n",
    "            orientation=\"h\",\n",
    "            error_x=dict(\n",
    "                type=\"data\",\n",
    "                symmetric=False,\n",
    "                array=coef[\"high\"] - coef[\"coef\"],\n",
    "                arrayminus=coef[\"coef\"] - coef[\"low\"],\n",
    "            ),\n",
    "            hovertemplate=\"%{y}<br>coef=%{x:.4f}<br>CI=[%{customdata[0]:.4f}, %{customdata[1]:.4f}]<extra></extra>\",\n",
    "            customdata=np.stack([coef[\"low\"], coef[\"high\"]], axis=-1),\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=PLOTLY_TEMPL,\n",
    "        xaxis_title=\"Coefficient (log-HR)\",\n",
    "        yaxis_title=\"Covariate\",\n",
    "        height=400,\n",
    "        title=\"Cox model: which factors hasten resubmission?\",\n",
    "    )\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Cox bar → {html_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a0814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FUNCS = [\n",
    "    speed_carpet,\n",
    "    rolling_median_gap_series,\n",
    "    monthly_resubmission_volume,\n",
    "    exporter_bar_race,\n",
    "    edge_rank_bump_chart,\n",
    "    attention_span_trend,\n",
    "    survival_interactive,\n",
    "    gap_ccdf,\n",
    "    hour_heatmap_interactive,\n",
    "    cox_coeff_bar_ci,\n",
    "]\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start = datetime.now()\n",
    "        for fn in ALL_FUNCS:\n",
    "            fname = fn.__name__\n",
    "            try:\n",
    "                print(f\"→ {fname}()\")\n",
    "                if fname == \"sankey_top_images\":\n",
    "                    fn(n_images=10)\n",
    "                else:\n",
    "                    fn()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ {fname} failed: {e}\")\n",
    "        print(\n",
    "            f\"Completed in {datetime.now() - start} – outputs in {FIG_DIR} & {HTML_DIR}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5181486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ speed_carpet()\n",
      "✔ Speed carpet → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\speed_carpet.html\n",
      "→ rolling_median_gap_series()\n",
      "→ monthly_resubmission_volume()\n",
      "→ exporter_bar_race()\n",
      "✔ Bar‑race → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\bar_race.html\n",
      "→ edge_rank_bump_chart()\n",
      "✔ Bump chart → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\edge_bump.html\n",
      "→ attention_span_trend()\n",
      "✔ Attention-span chart → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\attention_span.html\n",
      "→ survival_interactive()\n",
      "✔ Interactive KM → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\survival_interactive.html\n",
      "→ gap_ccdf()\n",
      "✔ CCDF → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\gap_ccdf.html\n",
      "→ hour_heatmap_interactive()\n",
      "✔ Hour heat-map → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\hour_heatmap.html\n",
      "→ cox_coeff_bar_ci()\n",
      "✔ Cox bar → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\\cox_coeffs_ci.html\n",
      "Completed in 0:00:22.662577 – outputs in C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal & C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\temporal\\html\n"
     ]
    }
   ],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
