{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf2b9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a190b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import pathlib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from pyvis.network import Network\n",
    "from upsetplot import UpSet, from_memberships\n",
    "\n",
    "try:\n",
    "    import community as community_louvain\n",
    "except ImportError:\n",
    "    community_louvain = None\n",
    "\n",
    "try:\n",
    "    from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "except ImportError:\n",
    "    KaplanMeierFitter = CoxPHFitter = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.distance import Levenshtein\n",
    "except ImportError:\n",
    "    Levenshtein = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a889abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = pathlib.Path(\".\").resolve().parent\n",
    "GRAPHS_DIR = ROOT / \"graphs\"\n",
    "PROC_DIR = ROOT / \"processed\"\n",
    "FIG_DIR = ROOT / \"figures\" / \"networks\"\n",
    "HTML_DIR = FIG_DIR / \"html\"\n",
    "\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "HTML_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLOTLY_TEMPL = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0139e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(name: str) -> nx.Graph:\n",
    "    \"\"\"Load a GraphML file and return a NetworkX graph.\"\"\"\n",
    "    path = GRAPHS_DIR / f\"{name}.graphml\"\n",
    "    return nx.read_graphml(path)\n",
    "\n",
    "\n",
    "def add_cluster_attribute(G: nx.Graph, resolution: float = 1.0, attr: str = \"cluster\"):\n",
    "    \"\"\"Add Louvain community IDs as a node attribute.\"\"\"\n",
    "    if community_louvain is None:\n",
    "        raise ImportError(\"python-louvain is not installed in this environment.\")\n",
    "    partition = community_louvain.best_partition(nx.Graph(G), resolution=resolution)\n",
    "    nx.set_node_attributes(G, partition, attr)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def load_hops() -> pd.DataFrame:\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\").sort_values(\n",
    "        [\"image_id\", \"unixtime\"]\n",
    "    )\n",
    "    df[\"next_time\"] = df.groupby(\"image_id\").unixtime.shift(-1)\n",
    "    df[\"next_sub\"] = df.groupby(\"image_id\").subreddit.shift(-1)\n",
    "    hops = df.dropna(subset=[\"next_time\"]).loc[df.subreddit != df.next_sub].copy()\n",
    "    hops[\"gap_h\"] = (hops.next_time - hops.unixtime) / 3600.0\n",
    "    first_year = pd.to_datetime(\n",
    "        df.groupby(\"image_id\").unixtime.transform(\"first\"), unit=\"s\"\n",
    "    ).dt.year\n",
    "    hops[\"first_year\"] = first_year\n",
    "    hops[\"gap_idx\"] = hops.groupby(\"image_id\").cumcount() + 1\n",
    "    hops[\"timestamp\"] = pd.to_datetime(hops.next_time, unit=\"s\")\n",
    "    return hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4 · LATENCY‑FLOW GRAPH & TIME‑TO‑EVENT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def latency_hist(plot_out: pathlib.Path = FIG_DIR / \"latency_hist.png\"):\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    gaps = [d[\"median_gap_h\"] for _, _, d in G.edges(data=True)]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(gaps, bins=60, log_scale=(False, True))\n",
    "    plt.xlabel(\"Median repost gap (h)\")\n",
    "    plt.ylabel(\"# edges (log)\")\n",
    "    plt.title(\"Distribution of median repost gaps\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Latency hist → {plot_out}\")\n",
    "\n",
    "\n",
    "def speed_carpet(html_out: pathlib.Path = HTML_DIR / \"speed_carpet.html\"):\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    add_cluster_attribute(G)\n",
    "    df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"src\": u,\n",
    "                \"dst\": v,\n",
    "                \"speed\": d[\"speed\"],\n",
    "                \"src_cl\": G.nodes[u][\"cluster\"],\n",
    "                \"dst_cl\": G.nodes[v][\"cluster\"],\n",
    "            }\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        ]\n",
    "    )\n",
    "    df_piv = df.pivot(index=\"src\", columns=\"dst\", values=\"speed\")\n",
    "    fig = px.imshow(\n",
    "        np.log10(df_piv.fillna(1e-6)),\n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale=\"Turbo\",\n",
    "        labels=dict(color=\"log10(speed)\"),\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=900,\n",
    "        title=\"Edge speed carpet (log10)\",\n",
    "    )\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.write_html(html_out)\n",
    "    print(f\"✔ Speed carpet → {html_out}\")\n",
    "\n",
    "\n",
    "def violin_latency_hubs(\n",
    "    plot_out: pathlib.Path = FIG_DIR / \"violin_latency_hubs.png\", top_n: int = 10\n",
    "):\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    hubs = sorted(G.in_degree(weight=\"n_hops\"), key=lambda kv: kv[1], reverse=True)[\n",
    "        :top_n\n",
    "    ]\n",
    "    hub_set = {n for n, _ in hubs}\n",
    "    data = [\n",
    "        {\"hub\": v, \"gap\": d[\"median_gap_h\"]}\n",
    "        for u, v, d in G.edges(data=True)\n",
    "        if v in hub_set\n",
    "    ]\n",
    "    df = pd.DataFrame(data)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.violinplot(data=df, x=\"hub\", y=\"gap\", scale=\"width\", inner=\"quartile\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Median gap distribution for revival hubs\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Violin → {plot_out}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4A · Survival curves per edge‑speed tercile\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def survival_curves_speed_tercile(\n",
    "    plot_out: pathlib.Path = FIG_DIR / \"survival_speed.png\",\n",
    "):\n",
    "    if KaplanMeierFitter is None:\n",
    "        print(\"⚠ lifelines not installed – survival curves skipped\")\n",
    "        return\n",
    "    G = load_graph(\"latency_flow\")\n",
    "    speed_vals = [d[\"speed\"] for _, _, d in G.edges(data=True)]\n",
    "    q1, q2 = np.quantile(speed_vals, [1 / 3, 2 / 3])\n",
    "    speed_cls = {\n",
    "        (u, v): (\"slow\" if s <= q1 else \"fast\" if s >= q2 else \"mid\")\n",
    "        for u, v, s in [(u, v, d[\"speed\"]) for u, v, d in G.edges(data=True)]\n",
    "    }\n",
    "\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    df = df.sort_values([\"image_id\", \"unixtime\"])\n",
    "    df[\"next_time\"] = df.groupby(\"image_id\")[\"unixtime\"].shift(-1)\n",
    "    df[\"next_sub\"] = df.groupby(\"image_id\")[\"subreddit\"].shift(-1)\n",
    "    hops = df.dropna(subset=[\"next_time\"]).loc[df.subreddit != df.next_sub]\n",
    "    hops[\"gap_h\"] = (hops.next_time - hops.unixtime) / 3600.0\n",
    "    hops[\"edge\"] = list(zip(hops.subreddit, hops.next_sub))\n",
    "    hops[\"speed_class\"] = hops.edge.map(speed_cls)\n",
    "    hops = hops.dropna(subset=[\"speed_class\"])  # drop hops not in latency graph\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for grp, dfg in hops.groupby(\"speed_class\"):\n",
    "        kmf.fit(durations=dfg.gap_h, event_observed=np.ones(len(dfg)), label=grp)\n",
    "        kmf.plot(ci_show=False)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Hours (log)\")\n",
    "    plt.ylabel(\"Survival probability\")\n",
    "    plt.title(\"Kaplan–Meier survival by edge‑speed tercile\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Survival curves → {plot_out}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4B · Cox proportional‑hazard model\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def cox_hazard_model(\n",
    "    plot_out: pathlib.Path = FIG_DIR / \"cox_coeffs.png\",\n",
    "    table_out: pathlib.Path = FIG_DIR / \"cox_summary.csv\",\n",
    "):\n",
    "    if CoxPHFitter is None or Levenshtein is None:\n",
    "        print(\"⚠ lifelines or rapidfuzz missing – Cox model skipped\")\n",
    "        return\n",
    "    df = pd.read_parquet(PROC_DIR / \"submissions_final.parquet\")\n",
    "    df = df.sort_values([\"image_id\", \"unixtime\"])\n",
    "    df[\"next_time\"] = df.groupby(\"image_id\")[\"unixtime\"].shift(-1)\n",
    "    df[\"next_score\"] = df.groupby(\"image_id\")[\"score\"].shift(-1)\n",
    "    df[\"next_title\"] = df.groupby(\"image_id\")[\"title\"].shift(-1)\n",
    "    df[\"next_sub\"] = df.groupby(\"image_id\")[\"subreddit\"].shift(-1)\n",
    "    hops = df.dropna(subset=[\"next_time\"]).loc[df.subreddit != df.next_sub]\n",
    "    hops[\"duration\"] = (hops.next_time - hops.unixtime) / 3600.0\n",
    "    hops[\"event\"] = 1  # always observed\n",
    "    # covariates\n",
    "    speed_lookup = {\n",
    "        (u, v): d[\"speed\"] for u, v, d in load_graph(\"latency_flow\").edges(data=True)\n",
    "    }\n",
    "    hops[\"speed\"] = list(\n",
    "        map(\n",
    "            lambda x: speed_lookup.get((x[0], x[1]), np.nan),\n",
    "            zip(hops.subreddit, hops.next_sub),\n",
    "        )\n",
    "    )\n",
    "    hops = hops.dropna(subset=[\"speed\"])\n",
    "    hops[\"title_dist\"] = hops.apply(\n",
    "        lambda r: Levenshtein.distance(r.title, r.next_title), axis=1\n",
    "    )\n",
    "    hops[\"init_score\"] = hops.score\n",
    "    cox_df = hops[[\"duration\", \"event\", \"speed\", \"title_dist\", \"init_score\"]].copy()\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(cox_df, duration_col=\"duration\", event_col=\"event\")\n",
    "    coef = cph.params_\n",
    "    coef.to_csv(table_out)\n",
    "    # barplot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    coef.sort_values().plot(kind=\"barh\")\n",
    "    plt.xlabel(\"Coefficient (log‑HR)\")\n",
    "    plt.title(\"Cox model coefficients\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"✔ Cox model → {plot_out}  (coeff table → {table_out})\")\n",
    "\n",
    "\n",
    "def half_life_trend_line(out=FIG_DIR / \"half_life_trend.png\"):\n",
    "    hops = load_hops()\n",
    "    mid = hops.groupby(\"image_id\").gap_h.median().reset_index(name=\"med\")\n",
    "    yrs = hops.groupby(\"image_id\").first_year.first().reset_index()\n",
    "    df = mid.merge(yrs)\n",
    "    trend = df.groupby(\"first_year\").med.median().reset_index()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(data=trend, x=\"first_year\", y=\"med\", marker=\"o\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"First‑year cohort\")\n",
    "    plt.ylabel(\"Median gap (h, log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gap_index_boxplot(out=FIG_DIR / \"gap_index_boxplot.png\", mx=5):\n",
    "    hops = load_hops()\n",
    "    flt = hops[hops.gap_idx <= mx]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=flt, x=\"gap_idx\", y=\"gap_h\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Repost index\")\n",
    "    plt.ylabel(\"Gap (h, log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def survival_curves_cohort_year(out=FIG_DIR / \"survival_cohort.png\"):\n",
    "    if KaplanMeierFitter is None:\n",
    "        print(\"ℹ️  lifelines not installed – survival by cohort skipped\")\n",
    "        return\n",
    "    hops = load_hops()\n",
    "    km = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for y, dfy in hops.groupby(\"first_year\"):\n",
    "        km.fit(durations=dfy.gap_h, event_observed=np.ones(len(dfy)), label=str(int(y)))\n",
    "        km.plot(ci_show=False)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Hours (log)\")\n",
    "    plt.ylabel(\"Survival\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def rolling_median_gap_series(out=HTML_DIR / \"rolling_gap.html\", window=\"30D\"):\n",
    "    hops = load_hops().set_index(\"timestamp\").sort_index()\n",
    "    med = hops.gap_h.rolling(window).median().dropna()\n",
    "    px.line(\n",
    "        med,\n",
    "        title=f\"Rolling {window} median repost gap\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=600,\n",
    "    ).update_yaxes(type=\"log\").write_html(out)\n",
    "\n",
    "\n",
    "def hourly_gap_heatmap(out=FIG_DIR / \"hourly_gap_heatmap.png\"):\n",
    "    hops = load_hops()\n",
    "    hops[\"src_hr\"] = pd.to_datetime(hops.unixtime, unit=\"s\").dt.hour\n",
    "    hops[\"dst_hr\"] = hops.timestamp.dt.hour\n",
    "    heat = hops.pivot_table(\n",
    "        values=\"gap_h\", index=\"src_hr\", columns=\"dst_hr\", aggfunc=\"median\"\n",
    "    )\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(\n",
    "        np.log10(heat + 1e-3), cmap=\"viridis\", cbar_kws={\"label\": \"log10(median gap h)\"}\n",
    "    )\n",
    "    plt.xlabel(\"Dest post hour\")\n",
    "    plt.ylabel(\"Source post hour\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def monthly_resubmission_volume(out=HTML_DIR / \"monthly_resubs.html\"):\n",
    "    hops = load_hops()\n",
    "    hops[\"month\"] = hops.timestamp.dt.to_period(\"M\").dt.to_timestamp()\n",
    "    vol = hops.groupby(\"month\").size().reset_index(name=\"resubs\")\n",
    "    px.bar(\n",
    "        vol,\n",
    "        x=\"month\",\n",
    "        y=\"resubs\",\n",
    "        title=\"Monthly resubmission volume\",\n",
    "        template=PLOTLY_TEMPL,\n",
    "        height=600,\n",
    "    ).write_html(out)\n",
    "\n",
    "\n",
    "def gap_distribution_by_year_violin(out=FIG_DIR / \"gap_year_violin.png\"):\n",
    "    hops = load_hops()\n",
    "    hops[\"year\"] = hops.timestamp.dt.year\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.violinplot(data=hops, x=\"year\", y=\"gap_h\", scale=\"width\", inner=\"quartile\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Gap (h, log)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FUNCS = [\n",
    "    latency_hist,\n",
    "    speed_carpet,\n",
    "    violin_latency_hubs,\n",
    "    survival_curves_speed_tercile,\n",
    "    cox_hazard_model,\n",
    "    half_life_trend_line,\n",
    "    gap_index_boxplot,\n",
    "    survival_curves_cohort_year,\n",
    "    rolling_median_gap_series,\n",
    "    hourly_gap_heatmap,\n",
    "    monthly_resubmission_volume,\n",
    "    gap_distribution_by_year_violin,\n",
    "]\n",
    "\n",
    "\n",
    "def run_all():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start = datetime.now()\n",
    "        for fn in ALL_FUNCS:\n",
    "            fname = fn.__name__\n",
    "            try:\n",
    "                print(f\"→ {fname}()\")\n",
    "                if fname == \"sankey_top_images\":\n",
    "                    fn(n_images=10)\n",
    "                else:\n",
    "                    fn()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ {fname} failed: {e}\")\n",
    "        print(\n",
    "            f\"Completed in {datetime.now() - start} – outputs in {FIG_DIR} & {HTML_DIR}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5181486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ latency_hist()\n",
      "✔ Latency hist → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\latency_hist.png\n",
      "→ speed_carpet()\n",
      "✔ Speed carpet → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\\speed_carpet.html\n",
      "→ violin_latency_hubs()\n",
      "✔ Violin → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\violin_latency_hubs.png\n",
      "→ survival_curves_speed_tercile()\n",
      "✔ Survival curves → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\survival_speed.png\n",
      "→ cox_hazard_model()\n",
      "✔ Cox model → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\cox_coeffs.png  (coeff table → C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\cox_summary.csv)\n",
      "→ half_life_trend_line()\n",
      "→ gap_index_boxplot()\n",
      "→ survival_curves_cohort_year()\n",
      "→ rolling_median_gap_series()\n",
      "→ hourly_gap_heatmap()\n",
      "→ monthly_resubmission_volume()\n",
      "→ gap_distribution_by_year_violin()\n",
      "Completed in 0:00:24.319407 – outputs in C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks & C:\\Users\\balsr\\OneDrive\\Desktop\\RedditDataAnalysis\\figures\\networks\\html\n"
     ]
    }
   ],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
